import aiohttp
import asyncio
import urllib.parse
from bs4 import BeautifulSoup
import math
import random
import json
import os
from itertools import product
from fake_useragent import UserAgent
import random
import re
from datetime import datetime

# ë¹„ë™ê¸° ì„¸ë§ˆí¬ì–´ ì„¤ì •
semaphore = asyncio.Semaphore(5)

# ì„ì‹œ ì €ì¥ íŒŒì¼ ì´ë¦„
PROGRESS_FILE = "data/progress.jsonl"

# ê±°ë˜ ìœ í˜• ë§¤í•‘
TRADE_TYPE_MAP = {
    "ë§¤ë§¤": "A1",
    "ì „ì„¸": "B1",
    "ì›”ì„¸": "B2",
    "ë‹¨ê¸°ì„ëŒ€": "B3",
}

# ë¶€ë™ì‚° ìœ í˜• ë§¤í•‘
PROPERTY_TYPE_MAP = {
    "ì•„íŒŒíŠ¸": "APT",
    "ì˜¤í”¼ìŠ¤í…”": "OPST",
    "ë¹Œë¼": "VL",
    "ì›ë£¸": "OR",
}

# ì¤‘ê°„ ì €ì¥ ë° ë¡œë“œ
def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"processed_articles": []}


def save_article(article):
    with open(PROGRESS_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(article, ensure_ascii=False) + "\n")


# ë¹„ë™ê¸° HTTP ìš”ì²­
async def fetch(session, url):
    try:
        async with session.get(url) as response:
            response.raise_for_status()
            return await response.text()
    except Exception as e:
        print(f"Fetch Error: {url}\n{e}")
        return None


async def fetch_json(session, url):
    try:
        async with session.get(url) as response:
            response.raise_for_status()
            return await response.json()
    except Exception as e:
        print(f"Fetch JSON Error: {url}\n{e}")
        return None


# URL ì¸ì½”ë”©
def encode_keyword(keyword):
    return urllib.parse.quote(keyword)


# ìœ„ì¹˜ ì •ë³´ íŒŒì‹±
def extract_location_info(response_text):
    soup = str(BeautifulSoup(response_text, "lxml"))
    try:
        value = (
            soup.split("filter: {")[1].split("}")[0].replace(" ", "").replace("'", "")
        )
    except IndexError:
        raise ValueError("ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return {
        "lat": value.split("lat:")[1].split(",")[0],
        "lon": value.split("lon:")[1].split(",")[0],
        "z": value.split("z:")[1].split(",")[0],
        "cortarNo": value.split("cortarNo:")[1].split(",")[0],
    }


# í´ëŸ¬ìŠ¤í„° URL ìƒì„±
def build_cluster_url(
    location_info, property_code, trade_code, lat_margin=0.5, lon_margin=0.5
):
    lat = float(location_info["lat"])
    lon = float(location_info["lon"])
    return (
        "https://m.land.naver.com/cluster/clusterList?"
        f"view=atcl&cortarNo={location_info['cortarNo']}&"
        f"rletTpCd={property_code}&tradTpCd={trade_code}&z={location_info['z']}&"
        f"lat={lat}&lon={lon}&"
        f"btm={lat - lat_margin}&lft={lon - lon_margin}&"
        f"top={lat + lat_margin}&rgt={lon + lon_margin}"
    )


# ìƒì„¸ ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ URL ìƒì„±
def build_article_list_url(
    lgeo, z, lat, lon, count, cortarNo, page, property_code, trade_code
):
    return (
        "https://m.land.naver.com/cluster/ajax/articleList?"
        f"itemId={lgeo}&mapKey=&lgeo={lgeo}&showR0=&"
        f"rletTpCd={property_code}&tradTpCd={trade_code}&z={z}&"
        f"lat={lat}&lon={lon}&totCnt={count}&"
        f"cortarNo={cortarNo}&page={page}"
    )


def refine_summary(item):
    base_img_url = "https://landthumb-phinf.pstatic.net"
    rep_img = item.get("repImgUrl")
    full_rep_img = f"{base_img_url}{rep_img}" if rep_img else None

    return {
        "ë§¤ë¬¼ë²ˆí˜¸": item.get("atclNo"),
        "í–‰ì •êµ¬ì—­ì½”ë“œ": item.get("cortarNo"),
        "ë§¤ë¬¼ëª…": item.get("atclNm"),
        "ë§¤ë¬¼ìƒíƒœ": item.get("atclStatCd"),
        "ë§¤ë¬¼ìœ í˜•ì½”ë“œ": item.get("rletTpCd"),
        "ë§¤ë¬¼ìœ í˜•ëª…": item.get("rletTpNm"),
        "ê±°ë˜ìœ í˜•ì½”ë“œ": item.get("tradTpCd"),
        "ê±°ë˜ìœ í˜•ëª…": item.get("tradTpNm"),
        "í™•ì¸ìœ í˜•": item.get("vrfcTpCd"),
        "ì¸µì •ë³´": item.get("flrInfo"),
        "ë³´ì¦ê¸ˆ": item.get("prc"),
        "ì›”ì„¸": item.get("rentPrc"),
        "í•œê¸€ê°€ê²©": item.get("hanPrc"),
        "ê³µê¸‰ë©´ì ": item.get("spc1"),
        "ì „ìš©ë©´ì ": item.get("spc2"),
        "ë°©í–¥": item.get("direction"),
        "ë§¤ë¬¼í™•ì¸ì¼": item.get("atclCfmYmd"),
        "ëŒ€í‘œì´ë¯¸ì§€": full_rep_img,
        "ì¸ë„¤ì¼íƒ€ì…": item.get("repImgTpCd"),
        "ì¸ë„¤ì¼": item.get("repImgThumb"),
        "ìœ„ë„": item.get("lat"),
        "ê²½ë„": item.get("lng"),
        "ë§¤ë¬¼íŠ¹ì§•": item.get("atclFetrDesc"),
        "ë§¤ë¬¼íƒœê·¸": ", ".join(item.get("tagList", [])),
        "ì¤‘ê°œì‚¬ëª…": item.get("rltrNm"),
        "ì§ê±°ë˜ì—¬ë¶€": item.get("directTradYn"),
        "ì¤‘ê°œì‚¬ìˆ˜": item.get("cpCnt"),
        "ê´‘ê³ ë§¤ì²´ëª…": item.get("cpNm"),
        "ì¤‘ê°œì‚¬ë¬´ì†Œ": item.get("rltrNm"),
        "ì†Œìš”ì‹œê°„": item.get("minute"),
        "ìƒì„¸ì£¼ì†Œì—¬ë¶€": item.get("dtlAddrYn"),
        "ìƒì„¸ì£¼ì†Œ": item.get("dtlAddr"),
        "ê°€ìƒí˜„ì‹¤ë…¸ì¶œì—¬ë¶€": item.get("isVrExposed"),
    }

# ë§¤ë¬¼ ìƒì„¸ ì •ë³´ íŒŒì‹±
def extract_detail_info(soup):
    result = {}

    def get_text(selector):
        element = soup.select_one(selector)
        return element.text.strip() if element else None

    def get_all_texts(selector):
        return [el.text.strip() for el in soup.select(selector)]

    # ë§¤ë¬¼ ë²ˆí˜¸
    result["ë§¤ë¬¼ë²ˆí˜¸"] = get_text('li:-soup-contains("ë§¤ë¬¼ë²ˆí˜¸") .DataList_definition__d9KY1')

    # ê°€ê²© ì „ì²´
    result["ê°€ê²©"] = get_text(".ArticleSummary_info-price__BD9wv")

    # ë³´ì¦ê¸ˆ/ì›”ì„¸ ë¶„ë¦¬
    if result["ê°€ê²©"]:
        price_split = result["ê°€ê²©"].replace(",", "").split("/")
        if len(price_split) == 2:
            result["ë³´ì¦ê¸ˆ"] = price_split[0].strip()
            result["ì›”ì„¸"] = price_split[1].strip()
        else:
            result["ë³´ì¦ê¸ˆ"], result["ì›”ì„¸"] = None, None
    else:
        result["ë³´ì¦ê¸ˆ"] = result["ì›”ì„¸"] = None

    # ê´€ë¦¬ë¹„ â€” ê¸ˆì•¡ë§Œ
    maintenance_items = soup.select('li:has(.DataList_term__Tks7l:-soup-contains("ê´€ë¦¬ë¹„"))')
    result["ê´€ë¦¬ë¹„"] = None
    for item in maintenance_items:
        value = item.select_one(".DataList_definition__d9KY1").text.strip()
        if "ì›" in value or "ë§Œì›" in value:
            result["ê´€ë¦¬ë¹„"] = value
            result["ê´€ë¦¬ë¹„"] = result["ê´€ë¦¬ë¹„"].replace("ìƒì„¸ë³´ê¸°", "")
            break

    # ë©´ì 
    result["ê³µê¸‰ë©´ì "] = get_text('li:-soup-contains("ê³µê¸‰ë©´ì ") .DataList_definition__d9KY1')
    result["ì „ìš©ë©´ì "] = get_text('li:-soup-contains("ì „ìš©ë©´ì ") .DataList_definition__d9KY1')

    # ì¸µìˆ˜
    result["ì¸µìˆ˜"] = get_text('li:-soup-contains("ì¸µ") .DataList_definition__d9KY1')

    # ë°©í–¥
    result["ë°©í–¥"] = get_text('li:-soup-contains("í–¥") .DataList_definition__d9KY1')

    # ë°©/ìš•ì‹¤ ìˆ˜
    result["ë°©/ìš•ì‹¤"] = get_text('li:-soup-contains("ë°©/ìš•ì‹¤") .DataList_definition__d9KY1')

    # ë³µì¸µ ì—¬ë¶€
    result["ë³µì¸µì—¬ë¶€"] = get_text('li:-soup-contains("ë³µì¸µì—¬ë¶€") .DataList_definition__d9KY1')

    # ì…ì£¼ ê°€ëŠ¥ì¼
    result["ì…ì£¼ê°€ëŠ¥ì¼"] = get_text(
        'li:-soup-contains("ì…ì£¼ê°€ëŠ¥ì¼") .DataList_definition__d9KY1'
    )

    # ì„¤ëª…
    result["ë§¤ë¬¼ì„¤ëª…"] = get_text(".ArticleSummary_description__5iFRy")

    # ì´ë¯¸ì§€ URL ëª©ë¡
    result["ì´ë¯¸ì§€ëª©ë¡"] = [
        img["src"] for img in soup.select(".ThumbnailMapGroup_image__kG4k5")
    ]

    # ì¤‘ê°œì‚¬ ì •ë³´
    result["ì¤‘ê°œì‚¬ëª…"] = get_text(".ArticleAgent_broker-name__IrVqj")
    result["ì¤‘ê°œì‚¬ì‚¬ë¬´ì†Œ"] = get_text(".ArticleAgent_info-agent__tWe2j")

    broker_phones = soup.select(
        'li:-soup-contains("ì „í™”") .ArticleAgent_link-telephone__RPK6B'
    )
    result["ì¤‘ê°œì‚¬ì „í™”"] = [phone.text.strip() for phone in broker_phones]

    result["ì¤‘ê°œì‚¬ì£¼ì†Œ"] = get_text('li:-soup-contains("ìœ„ì¹˜") .DataList_definition__d9KY1')
    result["ì¤‘ê°œì‚¬ë“±ë¡ë²ˆí˜¸"] = get_text(
        'li:-soup-contains("ë“±ë¡ë²ˆí˜¸") .DataList_definition__d9KY1'
    )
    result["ì¤‘ê°œì‚¬í™•ì¸ë§¤ë¬¼ìˆ˜"] = get_text(
        'li:-soup-contains("ì¤‘ê°œì‚¬ ë§¤ë¬¼") .DataList_definition__d9KY1'
    )

    return result

# ë§¤ë¬¼ ì €ì¥ í˜•íƒœ ë³€í™˜
def transform_article_to_property_row(article_data):
    summary = article_data["summary"]
    detail = article_data["detail"]

    def parse_floor(floor_str):
        try:
            return int(floor_str.split("/")[0].replace("ì¸µ", "").strip())
        except:
            return None
    
    def check_top_floor(floor_str):
        if (floor_str.split("/")[0].replace("ì¸µ", "").strip()) == "ê³ ":
            return True
        return False
    
    def check_bottom_floor(floor_str):
        if (floor_str.split("/")[0].replace("ì¸µ", "").strip()) == "ì €":
            return True
        return False
    
    def parse_all_floor(floor_str):
        try:
            return int(floor_str.split("/")[1].replace("ì¸µ", "").strip())
        except:
            return None

    def parse_confirmation_date(date_str):
        try:
            # ì˜ˆ: "25.04.19." â†’ "2025-04-19"
            date_str = date_str.strip().replace(".", "-").strip("-")
            return datetime.strptime("20" + date_str, "%Y-%m-%d").date()
        except:
            return None
    
    def parse_korean_currency(value):
        if not value:  # None, '', 0 ë“± ë¹„ì–´ìˆì„ ê²½ìš°
            return 0

        value = str(value).replace("ì›", "").replace(" ", "").replace(",", "").strip()

        try:
            return int(re.sub(r"[^\d]", "", value)) if re.search(r"\d", value) else 0
        except ValueError:
            print(f"âš ï¸ [ê´€ë¦¬ë¹„ íŒŒì‹± ì‹¤íŒ¨] ì›ë³¸ ê°’: {value}")
            return 0
        

    return {
        "monthly_rent_cost": int(summary.get("ì›”ì„¸", 0)),
        "deposit": int(summary.get("ë³´ì¦ê¸ˆ", 0)),
        "area": float(summary.get("ì „ìš©ë©´ì ", 0)),
        "floor": parse_floor(summary.get("ì¸µì •ë³´", "")),
        "total_floors": parse_all_floor(summary.get("ì¸µì •ë³´", "")),
        "is_top_floor": check_top_floor(summary.get("ì¸µì •ë³´", "")),
        "is_bottom_floor": check_bottom_floor(summary.get("ì¸µì •ë³´", "")),
        "property_type": summary.get("ë§¤ë¬¼ìœ í˜•ëª…"),
        "features": summary.get("ë§¤ë¬¼íŠ¹ì§•"),
        "direction": summary.get("ë°©í–¥"),
        "location": f"POINT({summary['ê²½ë„']} {summary['ìœ„ë„']})",
        "description": detail.get("ë§¤ë¬¼ì„¤ëª…"),
        "agent_name": detail.get("ì¤‘ê°œì‚¬ëª…"),
        "agent_office": detail.get("ì¤‘ê°œì‚¬ì‚¬ë¬´ì†Œ"),
        "agent_phone": ", ".join(detail.get("ì¤‘ê°œì‚¬ì „í™”", [])),
        "agent_address": detail.get("ì¤‘ê°œì‚¬ì£¼ì†Œ"),
        "agent_registration_no": detail.get("ì¤‘ê°œì‚¬ë“±ë¡ë²ˆí˜¸"),
        "property_number": summary.get("ë§¤ë¬¼ë²ˆí˜¸"),
        "administrative_code": summary.get("í–‰ì •êµ¬ì—­ì½”ë“œ"),
        "property_name": summary.get("ë§¤ë¬¼ëª…"),
        "transaction_type": summary.get("ê±°ë˜ìœ í˜•ëª…"),
        "confirmation_type": summary.get("í™•ì¸ìœ í˜•"),
        "supply_area": float(summary.get("ê³µê¸‰ë©´ì ", 0)),
        "property_confirmation_date": parse_confirmation_date(summary.get("ë§¤ë¬¼í™•ì¸ì¼", "")).isoformat(),
        "main_image_url": summary.get("ëŒ€í‘œì´ë¯¸ì§€", ""),
        "photo": detail.get("ì´ë¯¸ì§€ëª©ë¡", []),
        "tags": summary.get("ë§¤ë¬¼íƒœê·¸", ""),
        "maintenance_cost": parse_korean_currency(detail.get("ê´€ë¦¬ë¹„", "0ì›")),
        "rooms_bathrooms": detail.get("ë°©/ìš•ì‹¤"),
        "duplex": detail.get("ë³µì¸µì—¬ë¶€", "").strip() != "ë‹¨ì¸µ" if detail.get("ë³µì¸µì—¬ë¶€") else False,
        "available_move_in_date": detail.get("ì…ì£¼ê°€ëŠ¥ì¼", ""),
    }

# ë§¤ë¬¼ ìƒì„¸ ì •ë³´
async def fetch_article_detail(session, atcl_no, list_item, existing_atcl_numbers):
    async with semaphore:
        if atcl_no in existing_atcl_numbers:
            print(f"ğŸš« [ì¤‘ë³µ ê±´ë„ˆëœ€] atclNo: {atcl_no}")
            return

        url = f"https://m.land.naver.com/article/info/{atcl_no}"
        response_text = await fetch(session, url)

        useful_info = {}
        if response_text:
            soup = BeautifulSoup(response_text, "lxml")
            useful_info = extract_detail_info(soup)

        refined_summary = refine_summary(list_item)

        article_data = {
            "atclNo": atcl_no,
            "summary": refined_summary,
            "detail": useful_info
        }
        
        property_row = transform_article_to_property_row(article_data)

        save_article(property_row)
        existing_atcl_numbers.add(atcl_no)

        print(f"âœ… [ë§¤ë¬¼ ì €ì¥] atclNo: {atcl_no}")

        await asyncio.sleep(random.uniform(0.5, 1.5))

# ë§¤ë¬¼ ëª©ë¡ ì²˜ë¦¬
async def process_articles(session, article_json, existing_atcl_numbers):
    tasks = []
    body = article_json.get("body", [])
    if not body:
        return

    for item in body:
        atcl_no = item.get("atclNo")
        if atcl_no:
            tasks.append(fetch_article_detail(session, atcl_no, item, existing_atcl_numbers))

    await asyncio.gather(*tasks)

# í´ëŸ¬ìŠ¤í„° í•˜ë‚˜ ì²˜ë¦¬
async def process_single_cluster(session, cluster, cortarNo, property_code, trade_code, existing_atcl_numbers):
    lgeo = cluster["lgeo"]
    count = cluster["count"]
    z = cluster["z"]
    lat = cluster["lat"]
    lon = cluster["lon"]
    total_pages = math.ceil(count / 20)

    print(f"\n[í´ëŸ¬ìŠ¤í„°] lgeo: {lgeo}, ì´ ë§¤ë¬¼: {count}, í˜ì´ì§€: {total_pages}")

    for page in range(1, total_pages + 1):
        article_url = build_article_list_url(lgeo, z, lat, lon, count, cortarNo, page, property_code, trade_code)
        article_json = await fetch_json(session, article_url)

        if article_json:
            await process_articles(session, article_json, existing_atcl_numbers)

        await asyncio.sleep(random.uniform(0.5, 1.5))

# í´ëŸ¬ìŠ¤í„° ë³‘ë ¬ ì²˜ë¦¬
async def process_clusters(session, clusters, cortarNo, property_code, trade_code, existing_atcl_numbers):
    tasks = []

    for cluster in clusters:
        tasks.append(process_single_cluster(session, cluster, cortarNo, property_code, trade_code, existing_atcl_numbers))

    await asyncio.gather(*tasks)

# ì¡°í•© ì²˜ë¦¬
async def process_combinations(session, location_info, property_types, trade_types, existing_atcl_numbers):
    combinations = list(product(property_types, trade_types))
    for property_type, trade_type in combinations:
        property_code = PROPERTY_TYPE_MAP.get(property_type, "OR")
        trade_code = TRADE_TYPE_MAP.get(trade_type, "B2")

        print(f"\n[ì¡°í•©] ë¶€ë™ì‚°ìœ í˜•: {property_type} ({property_code}), ê±°ë˜ìœ í˜•: {trade_type} ({trade_code})")

        cluster_url = build_cluster_url(location_info, property_code, trade_code)
        cluster_json = await fetch_json(session, cluster_url)

        if not cluster_json or "data" not in cluster_json or "ARTICLE" not in cluster_json["data"]:
            print("í´ëŸ¬ìŠ¤í„° ì—†ìŒ")
            continue

        clusters = cluster_json["data"]["ARTICLE"]
        await process_clusters(session, clusters, location_info["cortarNo"], property_code, trade_code, existing_atcl_numbers)

# ê¸°ì¡´ ë§¤ë¬¼ ë²ˆí˜¸ ë¡œë“œ
def load_existing_atcl_numbers(filepath=PROGRESS_FILE):
    atcl_numbers = set()
    if not os.path.exists(filepath):
        return atcl_numbers

    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            try:
                article = json.loads(line)
                atcl_no = article.get("atclNo")
                if atcl_no:
                    atcl_numbers.add(atcl_no)
            except json.JSONDecodeError:
                continue

    print(f"âœ… ì´ë¯¸ ì €ì¥ëœ ë§¤ë¬¼ ìˆ˜: {len(atcl_numbers)}")
    return atcl_numbers

# ëœë¤ User-Agent ìƒì„±
ua = UserAgent()

def get_random_headers():
    # Accept-Language ë‹¤ì–‘í™”
    accept_language_list = [
        "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "en-US,en;q=0.9,ko;q=0.8",
        "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
        "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "en-GB,en;q=0.9",
    ]
    
    # Accept í—¤ë” ë‹¤ì–‘í™”
    accept_list = [
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "text/html,application/xml;q=0.9,image/webp,*/*;q=0.8",
    ]
    
    # Connection í—¤ë” ë‹¤ì–‘í™”
    connection_list = [
        "keep-alive",
        "close",
    ]
    
    # Referer ë‹¤ì–‘í™” (ì‹¤ì œ í˜ì´ì§€ íë¦„ì—ì„œ ìœ ë„)
    referer_list = [
        "https://m.land.naver.com/",
        "https://m.land.naver.com/map/37.5665:126.9780:15/0",
        "https://m.land.naver.com/search/result/ì„œìš¸",
        "https://m.land.naver.com/article/info/{}".format(random.randint(1000000000, 9999999999)),
    ]
    
    # Cache-Control ë‹¤ì–‘í™”
    cache_control_list = [
        "max-age=0",
        "no-cache",
        "no-store",
    ]
    
    # Upgrade-Insecure-Requests ë‹¤ì–‘í™”
    upgrade_insecure_list = ["1", "0"]

    # DNT (Do Not Track) ëœë¤
    dnt_list = ["1", "0"]

    return {
        "User-Agent": ua.random,
        "Accept-Language": random.choice(accept_language_list),
        "Accept": random.choice(accept_list),
        "Connection": random.choice(connection_list),
        "Referer": random.choice(referer_list),
        "Cache-Control": random.choice(cache_control_list),
        "Upgrade-Insecure-Requests": random.choice(upgrade_insecure_list),
        "DNT": random.choice(dnt_list),
        # ì‹¤ì œ ì¿ í‚¤ ì ìš© ì‹œ ë” ì¢‹ìŒ
        "Cookie": "cookie",  # ì‹¤ì œ ì¿ í‚¤ ìˆìœ¼ë©´ ë„£ê¸°!
    }

# ë©”ì¸ í•¨ìˆ˜
async def house_crawler(keyword):
    print(f"ğŸ  [ê²€ìƒ‰ì–´] {keyword}")
    existing_atcl_numbers = load_existing_atcl_numbers()
    cookie_jar = aiohttp.CookieJar()

    async with aiohttp.ClientSession(headers=get_random_headers(), cookie_jar=cookie_jar) as session:
        encoded_keyword = encode_keyword(keyword)
        search_url = f"https://m.land.naver.com/search/result/{encoded_keyword}"
        search_response_text = await fetch(session, search_url)

        if not search_response_text:
            print("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return

        location_info = extract_location_info(search_response_text)

        property_types = ["ì›ë£¸"]
        trade_types = ["ì›”ì„¸"]

        await process_combinations(session, location_info, property_types, trade_types, existing_atcl_numbers)

        print("ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(house_crawler("ì„œìš¸ì‹œ ê´‘ì§„êµ¬ ê´‘ì¥ë™"))
